{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Biobert_GLUE_All_FINAL.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"neIZ_n8D3C9p","outputId":"f16fcd59-66c7-444f-f875-f9382fbaf3cc"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HG18LZgU3NrH","outputId":"5a233c87-da98-4c79-f44a-8f501724eda0"},"source":["!git clone https://github.com/huggingface/transformers.git"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'transformers'...\n","remote: Enumerating objects: 70470, done.\u001b[K\n","remote: Counting objects: 100% (559/559), done.\u001b[K\n","remote: Compressing objects: 100% (347/347), done.\u001b[K\n","remote: Total 70470 (delta 279), reused 347 (delta 167), pack-reused 69911\u001b[K\n","Receiving objects: 100% (70470/70470), 53.73 MiB | 28.57 MiB/s, done.\n","Resolving deltas: 100% (49851/49851), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CqwuGh1d3R1V","outputId":"ced2190f-5e89-4793-dbfe-f9ec69406e7c"},"source":["cd transformers/examples"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/transformers/examples\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GDfu27zr3W1Z","outputId":"b02c12ae-6c17-41b0-a468-f29d23ab7b18"},"source":["ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[0m\u001b[01;34mbenchmarking\u001b[0m/        README.md                test_xla_examples.py\n","conftest.py          \u001b[01;34mresearch_projects\u001b[0m/       \u001b[01;34mtext-classification\u001b[0m/\n","\u001b[01;34mlanguage-modeling\u001b[0m/   \u001b[01;34mseq2seq\u001b[0m/                 \u001b[01;34mtext-generation\u001b[0m/\n","\u001b[01;34mlegacy\u001b[0m/              \u001b[01;34mtest_data\u001b[0m/               \u001b[01;34mtoken-classification\u001b[0m/\n","\u001b[01;34mmultiple-choice\u001b[0m/     test_examples.py         xla_spawn.py\n","\u001b[01;34mquestion-answering\u001b[0m/  _tests_requirements.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"52r5Voq43YKD","outputId":"598de117-252b-4c98-d3f3-7d3ec62a997f"},"source":["cd text-classification/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/transformers/examples/text-classification\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q0_n3rYN3Z23","outputId":"02ff522f-fac4-4e01-ea52-b45fcf2e0155"},"source":["ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["README.md               \u001b[0m\u001b[01;32mrun_glue.py\u001b[0m*                    \u001b[01;32mrun_xnli.py\u001b[0m*\n","requirements.txt        \u001b[01;32mrun_tf_glue.py\u001b[0m*\n","run_glue_no_trainer.py  \u001b[01;32mrun_tf_text_classification.py\u001b[0m*\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SdpUSXs43bWg","outputId":"539725c7-490b-4237-bc99-ef935276ba76"},"source":["! git clone https://github.com/dmis-lab/biobert-pytorch.git"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'biobert-pytorch'...\n","remote: Enumerating objects: 211, done.\u001b[K\n","remote: Counting objects: 100% (211/211), done.\u001b[K\n","remote: Compressing objects: 100% (184/184), done.\u001b[K\n","remote: Total 211 (delta 43), reused 169 (delta 15), pack-reused 0\u001b[K\n","Receiving objects: 100% (211/211), 1.91 MiB | 23.86 MiB/s, done.\n","Resolving deltas: 100% (43/43), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V8pUG6_p3gjZ","outputId":"93a37072-e470-4882-c444-9110f5297d6a"},"source":["!pwd\n","#Should be /content/transformers/examples/text-classification still"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/transformers/examples/text-classification\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yklNkR2E3vtx","outputId":"fcd6fca4-1828-4093-b209-d357dfa6f186"},"source":["!pip install datasets"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting datasets\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/d6/a3d2c55b940a7c556e88f5598b401990805fc0f0a28b2fc9870cf0b8c761/datasets-1.6.0-py3-none-any.whl (202kB)\n","\r\u001b[K     |█▋                              | 10kB 23.0MB/s eta 0:00:01\r\u001b[K     |███▎                            | 20kB 29.8MB/s eta 0:00:01\r\u001b[K     |████▉                           | 30kB 20.4MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 40kB 23.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 51kB 22.5MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 61kB 17.2MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 71kB 17.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 81kB 18.9MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 92kB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 102kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 112kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 122kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 133kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 143kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 153kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 163kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 174kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 184kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 194kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 204kB 18.1MB/s \n","\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (3.10.1)\n","Collecting huggingface-hub<0.1.0\n","  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n","Collecting xxhash\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n","\u001b[K     |████████████████████████████████| 245kB 53.5MB/s \n","\u001b[?25hRequirement already satisfied: pyarrow>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (20.9)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n","Collecting fsspec\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/91/2ef649137816850fa4f4c97c6f2eabb1a79bf0aa2c8ed198e387e373455e/fsspec-2021.4.0-py3-none-any.whl (108kB)\n","\u001b[K     |████████████████████████████████| 112kB 50.2MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: huggingface-hub, xxhash, fsspec, datasets\n","Successfully installed datasets-1.6.0 fsspec-2021.4.0 huggingface-hub-0.0.8 xxhash-2.0.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GKZW-rpB3yp9","outputId":"04be914a-ca85-4a8f-b3d5-1de3606b5203"},"source":["!pip install git+https://github.com/huggingface/transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/huggingface/transformers\n","  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-pp2wzltr\n","  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-pp2wzltr\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 19.8MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (4.41.1)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (3.10.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (20.9)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 50.6MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (1.19.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (1.0.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.6.0.dev0) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.6.0.dev0) (3.7.4.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (3.0.4)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.6.0.dev0) (2.4.7)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.6.0.dev0-cp37-none-any.whl size=2112590 sha256=2ba73c72197ef3780046a1624627a153ba98e59dcf73e194bcf76cfb12f3f5dd\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-joc71ia4/wheels/70/d3/52/b3fa4f8b8ef04167ac62e5bb2accb62ae764db2a378247490e\n","Successfully built transformers\n","Installing collected packages: sacremoses, tokenizers, transformers\n","Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.6.0.dev0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RfBYatq4gOeh"},"source":["# DO NOT RUN BELOW HERE TO SAVE RESULTS OF PREVIOUS GLUE"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ROg-NfpcCXnb","outputId":"262e0b55-1392-4861-c0a8-6c72c33d69e1"},"source":["!python run_glue.py \\\n","  --model_name_or_path dmis-lab/biobert-base-cased-v1.1 \\\n","  --task_name wnli \\\n","  --do_train \\\n","  --do_eval \\\n","  --max_seq_length 128 \\\n","  --per_device_train_batch_size 32 \\\n","  --learning_rate 2e-5 \\\n","  --num_train_epochs 3 \\\n","  --output_dir /tmp/wnli/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-04-21 01:28:30.721072: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n","04/21/2021 01:28:32 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","04/21/2021 01:28:32 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/tmp/wnli/, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Apr21_01-28-32_04e6c04e8023, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/tmp/wnli/, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, _n_gpu=1, mp_parameters=)\n","Downloading: 28.8kB [00:00, 24.5MB/s]       \n","Downloading: 28.7kB [00:00, 28.4MB/s]       \n","Downloading and preparing dataset glue/wnli (download: 28.32 KiB, generated: 154.03 KiB, post-processed: Unknown size, total: 182.35 KiB) to /root/.cache/huggingface/datasets/glue/wnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n","Downloading: 100% 29.0k/29.0k [00:00<00:00, 481kB/s]\n","Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/wnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n","[INFO|file_utils.py:1394] 2021-04-21 01:28:34,197 >> https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpr0v7t4ms\n","Downloading: 100% 313/313 [00:00<00:00, 487kB/s]\n","[INFO|file_utils.py:1398] 2021-04-21 01:28:34,214 >> storing https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/78e6e8ece5b58501028ce314273009ad7707ef4c5ba44251914fd6bca8a05eff.e4a2e693122d98b8b56b7dc1f0d89b644226aacef228afb5030ee3621b2829d3\n","[INFO|file_utils.py:1401] 2021-04-21 01:28:34,214 >> creating metadata file for /root/.cache/huggingface/transformers/78e6e8ece5b58501028ce314273009ad7707ef4c5ba44251914fd6bca8a05eff.e4a2e693122d98b8b56b7dc1f0d89b644226aacef228afb5030ee3621b2829d3\n","[INFO|configuration_utils.py:491] 2021-04-21 01:28:34,215 >> loading configuration file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/78e6e8ece5b58501028ce314273009ad7707ef4c5ba44251914fd6bca8a05eff.e4a2e693122d98b8b56b7dc1f0d89b644226aacef228afb5030ee3621b2829d3\n","[INFO|configuration_utils.py:527] 2021-04-21 01:28:34,215 >> Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": \"wnli\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.6.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|configuration_utils.py:491] 2021-04-21 01:28:34,232 >> loading configuration file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/78e6e8ece5b58501028ce314273009ad7707ef4c5ba44251914fd6bca8a05eff.e4a2e693122d98b8b56b7dc1f0d89b644226aacef228afb5030ee3621b2829d3\n","[INFO|configuration_utils.py:527] 2021-04-21 01:28:34,232 >> Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.6.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|file_utils.py:1394] 2021-04-21 01:28:34,249 >> https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmphcxgwf9x\n","Downloading: 100% 213k/213k [00:00<00:00, 12.6MB/s]\n","[INFO|file_utils.py:1398] 2021-04-21 01:28:34,284 >> storing https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/cda9c238619f77c4dbf65f23ab841b5239f0289800d673957c843b9dda52ad44.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|file_utils.py:1401] 2021-04-21 01:28:34,284 >> creating metadata file for /root/.cache/huggingface/transformers/cda9c238619f77c4dbf65f23ab841b5239f0289800d673957c843b9dda52ad44.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|tokenization_utils_base.py:1713] 2021-04-21 01:28:34,348 >> loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/cda9c238619f77c4dbf65f23ab841b5239f0289800d673957c843b9dda52ad44.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|tokenization_utils_base.py:1713] 2021-04-21 01:28:34,349 >> loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:1713] 2021-04-21 01:28:34,349 >> loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1713] 2021-04-21 01:28:34,349 >> loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1713] 2021-04-21 01:28:34,349 >> loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/tokenizer_config.json from cache at None\n","[INFO|file_utils.py:1394] 2021-04-21 01:28:34,407 >> https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmph28a33ut\n","Downloading: 100% 436M/436M [00:14<00:00, 30.2MB/s]\n","[INFO|file_utils.py:1398] 2021-04-21 01:28:48,860 >> storing https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/f22d3e86464eb4aa2e5f76e0560e9d595617f55e014c78f176b834f9245a62f0.2cf22e1fb1f2db88b971c8da5593b1e94a0ee79d8c892a0f6d4f607d2b77fa23\n","[INFO|file_utils.py:1401] 2021-04-21 01:28:48,860 >> creating metadata file for /root/.cache/huggingface/transformers/f22d3e86464eb4aa2e5f76e0560e9d595617f55e014c78f176b834f9245a62f0.2cf22e1fb1f2db88b971c8da5593b1e94a0ee79d8c892a0f6d4f607d2b77fa23\n","[INFO|modeling_utils.py:1075] 2021-04-21 01:28:48,860 >> loading weights file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/f22d3e86464eb4aa2e5f76e0560e9d595617f55e014c78f176b834f9245a62f0.2cf22e1fb1f2db88b971c8da5593b1e94a0ee79d8c892a0f6d4f607d2b77fa23\n","[WARNING|modeling_utils.py:1196] 2021-04-21 01:28:52,354 >> Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.1 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:1207] 2021-04-21 01:28:52,355 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","100% 1/1 [00:00<00:00,  8.14ba/s]\n","100% 1/1 [00:00<00:00, 82.96ba/s]\n","100% 1/1 [00:00<00:00, 35.50ba/s]\n","04/21/2021 01:28:52 - INFO - __main__ -   Sample 114 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 114, 'input_ids': [101, 1107, 179, 4654, 1183, 117, 24181, 1306, 6943, 2980, 1968, 3332, 1594, 1113, 11078, 4786, 23450, 119, 1290, 11078, 4786, 23450, 112, 188, 2306, 1108, 1277, 1618, 5440, 1105, 1995, 1551, 2610, 117, 1152, 1127, 2378, 1439, 2277, 119, 102, 11078, 4786, 23450, 1108, 2378, 1439, 2277, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 0, 'sentence1': \"In July, Kamtchatka declared war on Yakutsk. Since Yakutsk's army was much better equipped and ten times larger, they were defeated within weeks.\", 'sentence2': 'Yakutsk was defeated within weeks.', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n","04/21/2021 01:28:52 - INFO - __main__ -   Sample 25 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 25, 'input_ids': [101, 1103, 11073, 2144, 112, 189, 4218, 1154, 1103, 3058, 17655, 1272, 1122, 1110, 1315, 1415, 119, 102, 1103, 17655, 1110, 1315, 1415, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 0, 'sentence1': \"The trophy doesn't fit into the brown suitcase because it is too large.\", 'sentence2': 'The suitcase is too large.', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n","04/21/2021 01:28:52 - INFO - __main__ -   Sample 281 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 281, 'input_ids': [101, 185, 18318, 1793, 1106, 1840, 176, 25690, 2176, 1113, 1103, 2179, 117, 1133, 1119, 1445, 112, 189, 2265, 119, 102, 176, 25690, 2176, 1445, 112, 189, 2265, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 0, 'sentence1': \"Paul tried to call George on the phone, but he wasn't successful.\", 'sentence2': \"George wasn't successful.\", 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n","Downloading: 5.75kB [00:00, 6.27MB/s]       \n","[INFO|trainer.py:497] 2021-04-21 01:29:04,150 >> The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1.\n","[INFO|trainer.py:1107] 2021-04-21 01:29:04,156 >> ***** Running training *****\n","[INFO|trainer.py:1108] 2021-04-21 01:29:04,156 >>   Num examples = 635\n","[INFO|trainer.py:1109] 2021-04-21 01:29:04,156 >>   Num Epochs = 3\n","[INFO|trainer.py:1110] 2021-04-21 01:29:04,157 >>   Instantaneous batch size per device = 32\n","[INFO|trainer.py:1111] 2021-04-21 01:29:04,157 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n","[INFO|trainer.py:1112] 2021-04-21 01:29:04,157 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1113] 2021-04-21 01:29:04,157 >>   Total optimization steps = 60\n","100% 60/60 [00:41<00:00,  1.47it/s][INFO|trainer.py:1302] 2021-04-21 01:29:45,261 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 41.1044, 'train_samples_per_second': 1.46, 'epoch': 3.0}\n","100% 60/60 [00:41<00:00,  1.46it/s]\n","[INFO|trainer.py:1768] 2021-04-21 01:29:45,454 >> Saving model checkpoint to /tmp/wnli/\n","[INFO|configuration_utils.py:329] 2021-04-21 01:29:45,455 >> Configuration saved in /tmp/wnli/config.json\n","[INFO|modeling_utils.py:848] 2021-04-21 01:29:46,676 >> Model weights saved in /tmp/wnli/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1918] 2021-04-21 01:29:46,677 >> tokenizer config file saved in /tmp/wnli/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1924] 2021-04-21 01:29:46,677 >> Special tokens file saved in /tmp/wnli/special_tokens_map.json\n","[INFO|trainer_pt_utils.py:898] 2021-04-21 01:29:46,738 >> ***** train metrics *****\n","[INFO|trainer_pt_utils.py:903] 2021-04-21 01:29:46,738 >>   epoch                      =        3.0\n","[INFO|trainer_pt_utils.py:903] 2021-04-21 01:29:46,738 >>   init_mem_cpu_alloc_delta   =     2265MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-21 01:29:46,738 >>   init_mem_cpu_peaked_delta  =        0MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-21 01:29:46,738 >>   init_mem_gpu_alloc_delta   =      413MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-21 01:29:46,738 >>   init_mem_gpu_peaked_delta  =        0MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-21 01:29:46,738 >>   train_mem_cpu_alloc_delta  =       11MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-21 01:29:46,738 >>   train_mem_cpu_peaked_delta =        0MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-21 01:29:46,738 >>   train_mem_gpu_alloc_delta  =     1298MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-21 01:29:46,739 >>   train_mem_gpu_peaked_delta =     3395MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-21 01:29:46,739 >>   train_runtime              = 0:00:41.10\n","[INFO|trainer_pt_utils.py:903] 2021-04-21 01:29:46,739 >>   train_samples              =        635\n","[INFO|trainer_pt_utils.py:903] 2021-04-21 01:29:46,739 >>   train_samples_per_second   =       1.46\n","04/21/2021 01:29:46 - INFO - __main__ -   *** Evaluate ***\n","[INFO|trainer.py:497] 2021-04-21 01:29:46,847 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1.\n","[INFO|trainer.py:1988] 2021-04-21 01:29:46,850 >> ***** Running Evaluation *****\n","[INFO|trainer.py:1990] 2021-04-21 01:29:46,850 >>   Num examples = 71\n","[INFO|trainer.py:1993] 2021-04-21 01:29:46,850 >>   Batch size = 8\n","100% 9/9 [00:00<00:00, 18.30it/s]\n","[INFO|trainer_pt_utils.py:898] 2021-04-21 01:29:47,570 >> ***** eval metrics *****\n","[INFO|trainer_pt_utils.py:903] 2021-04-21 01:29:47,570 >>   epoch                     =        3.0\n","[INFO|trainer_pt_utils.py:903] 2021-04-21 01:29:47,570 >>   eval_accuracy             =     0.3944\n","[INFO|trainer_pt_utils.py:903] 2021-04-21 01:29:47,570 >>   eval_loss                 =     0.7072\n","[INFO|trainer_pt_utils.py:903] 2021-04-21 01:29:47,570 >>   eval_mem_cpu_alloc_delta  =        1MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-21 01:29:47,570 >>   eval_mem_cpu_peaked_delta =        0MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-21 01:29:47,570 >>   eval_mem_gpu_alloc_delta  =        0MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-21 01:29:47,570 >>   eval_mem_gpu_peaked_delta =       33MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-21 01:29:47,570 >>   eval_runtime              = 0:00:00.61\n","[INFO|trainer_pt_utils.py:903] 2021-04-21 01:29:47,570 >>   eval_samples              =         71\n","[INFO|trainer_pt_utils.py:903] 2021-04-21 01:29:47,570 >>   eval_samples_per_second   =    115.917\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dTx7xx_WV2Hu","outputId":"6cff7426-7a58-4e52-9407-d88f76210721"},"source":["!python run_glue.py \\\n","  --model_name_or_path dmis-lab/biobert-base-cased-v1.1 \\\n","  --task_name sst2 \\\n","  --do_train \\\n","  --do_eval \\\n","  --max_seq_length 128 \\\n","  --per_device_train_batch_size 32 \\\n","  --learning_rate 2e-5 \\\n","  --num_train_epochs 3 \\\n","  --output_dir /tmp/sst2/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-04-20 12:53:33.302640: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n","04/20/2021 12:53:35 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","04/20/2021 12:53:35 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/tmp/sst2/, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Apr20_12-53-35_873cfbb4fb8a, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/tmp/sst2/, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, _n_gpu=1, mp_parameters=)\n","Downloading: 28.7kB [00:00, 21.0MB/s]       \n","Downloading: 28.7kB [00:00, 23.4MB/s]       \n","Downloading and preparing dataset glue/sst2 (download: 7.09 MiB, generated: 4.81 MiB, post-processed: Unknown size, total: 11.90 MiB) to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4...\n","Downloading: 100% 7.44M/7.44M [00:00<00:00, 7.84MB/s]\n","Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4. Subsequent calls will reuse this data.\n","[INFO|file_utils.py:1394] 2021-04-20 12:53:39,510 >> https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpawv65s0w\n","Downloading: 100% 313/313 [00:00<00:00, 348kB/s]\n","[INFO|file_utils.py:1398] 2021-04-20 12:53:39,527 >> storing https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/78e6e8ece5b58501028ce314273009ad7707ef4c5ba44251914fd6bca8a05eff.e4a2e693122d98b8b56b7dc1f0d89b644226aacef228afb5030ee3621b2829d3\n","[INFO|file_utils.py:1401] 2021-04-20 12:53:39,527 >> creating metadata file for /root/.cache/huggingface/transformers/78e6e8ece5b58501028ce314273009ad7707ef4c5ba44251914fd6bca8a05eff.e4a2e693122d98b8b56b7dc1f0d89b644226aacef228afb5030ee3621b2829d3\n","[INFO|configuration_utils.py:491] 2021-04-20 12:53:39,527 >> loading configuration file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/78e6e8ece5b58501028ce314273009ad7707ef4c5ba44251914fd6bca8a05eff.e4a2e693122d98b8b56b7dc1f0d89b644226aacef228afb5030ee3621b2829d3\n","[INFO|configuration_utils.py:527] 2021-04-20 12:53:39,528 >> Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": \"sst2\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.6.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|configuration_utils.py:491] 2021-04-20 12:53:39,545 >> loading configuration file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/78e6e8ece5b58501028ce314273009ad7707ef4c5ba44251914fd6bca8a05eff.e4a2e693122d98b8b56b7dc1f0d89b644226aacef228afb5030ee3621b2829d3\n","[INFO|configuration_utils.py:527] 2021-04-20 12:53:39,545 >> Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.6.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|file_utils.py:1394] 2021-04-20 12:53:39,565 >> https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp1hdd3u8n\n","Downloading: 100% 213k/213k [00:00<00:00, 26.7MB/s]\n","[INFO|file_utils.py:1398] 2021-04-20 12:53:39,593 >> storing https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/cda9c238619f77c4dbf65f23ab841b5239f0289800d673957c843b9dda52ad44.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|file_utils.py:1401] 2021-04-20 12:53:39,593 >> creating metadata file for /root/.cache/huggingface/transformers/cda9c238619f77c4dbf65f23ab841b5239f0289800d673957c843b9dda52ad44.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|tokenization_utils_base.py:1713] 2021-04-20 12:53:39,662 >> loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/cda9c238619f77c4dbf65f23ab841b5239f0289800d673957c843b9dda52ad44.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|tokenization_utils_base.py:1713] 2021-04-20 12:53:39,662 >> loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:1713] 2021-04-20 12:53:39,662 >> loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1713] 2021-04-20 12:53:39,662 >> loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1713] 2021-04-20 12:53:39,662 >> loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/tokenizer_config.json from cache at None\n","[INFO|file_utils.py:1394] 2021-04-20 12:53:39,720 >> https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpay8zrcnx\n","Downloading: 100% 436M/436M [00:08<00:00, 49.9MB/s]\n","[INFO|file_utils.py:1398] 2021-04-20 12:53:48,744 >> storing https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/f22d3e86464eb4aa2e5f76e0560e9d595617f55e014c78f176b834f9245a62f0.2cf22e1fb1f2db88b971c8da5593b1e94a0ee79d8c892a0f6d4f607d2b77fa23\n","[INFO|file_utils.py:1401] 2021-04-20 12:53:48,745 >> creating metadata file for /root/.cache/huggingface/transformers/f22d3e86464eb4aa2e5f76e0560e9d595617f55e014c78f176b834f9245a62f0.2cf22e1fb1f2db88b971c8da5593b1e94a0ee79d8c892a0f6d4f607d2b77fa23\n","[INFO|modeling_utils.py:1075] 2021-04-20 12:53:48,745 >> loading weights file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/f22d3e86464eb4aa2e5f76e0560e9d595617f55e014c78f176b834f9245a62f0.2cf22e1fb1f2db88b971c8da5593b1e94a0ee79d8c892a0f6d4f607d2b77fa23\n","[WARNING|modeling_utils.py:1196] 2021-04-20 12:53:52,236 >> Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.1 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:1207] 2021-04-20 12:53:52,236 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","100% 68/68 [00:04<00:00, 13.90ba/s]\n","100% 1/1 [00:00<00:00, 10.43ba/s]\n","100% 2/2 [00:00<00:00, 11.62ba/s]\n","04/20/2021 12:53:57 - INFO - __main__ -   Sample 14592 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 14592, 'input_ids': [101, 170, 1632, 2523, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1, 'sentence': 'a great movie ', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n","04/20/2021 12:53:57 - INFO - __main__ -   Sample 3278 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 3278, 'input_ids': [101, 15021, 117, 1191, 4742, 18013, 117, 2168, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1, 'sentence': 'entertaining , if somewhat standardized , action ', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n","04/20/2021 12:53:57 - INFO - __main__ -   Sample 36048 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 36048, 'input_ids': [101, 1256, 1165, 1175, 1132, 181, 11781, 1116, 117, 1103, 6288, 3166, 16047, 117, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1, 'sentence': 'even when there are lulls , the emotions seem authentic , ', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n","Downloading: 5.75kB [00:00, 5.09MB/s]       \n","[INFO|trainer.py:502] 2021-04-20 12:54:08,848 >> The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence.\n","[INFO|trainer.py:502] 2021-04-20 12:54:08,848 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence.\n","[INFO|trainer.py:1102] 2021-04-20 12:54:09,066 >> ***** Running training *****\n","[INFO|trainer.py:1103] 2021-04-20 12:54:09,066 >>   Num examples = 67349\n","[INFO|trainer.py:1104] 2021-04-20 12:54:09,066 >>   Num Epochs = 3\n","[INFO|trainer.py:1105] 2021-04-20 12:54:09,066 >>   Instantaneous batch size per device = 32\n","[INFO|trainer.py:1106] 2021-04-20 12:54:09,067 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n","[INFO|trainer.py:1107] 2021-04-20 12:54:09,067 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1108] 2021-04-20 12:54:09,067 >>   Total optimization steps = 6315\n","{'loss': 0.406, 'learning_rate': 1.8416468725257326e-05, 'epoch': 0.24}\n","  8% 500/6315 [05:54<1:11:03,  1.36it/s][INFO|trainer.py:1763] 2021-04-20 13:00:03,623 >> Saving model checkpoint to /tmp/sst2/checkpoint-500\n","[INFO|configuration_utils.py:329] 2021-04-20 13:00:03,624 >> Configuration saved in /tmp/sst2/checkpoint-500/config.json\n","[INFO|modeling_utils.py:848] 2021-04-20 13:00:04,898 >> Model weights saved in /tmp/sst2/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1918] 2021-04-20 13:00:04,899 >> tokenizer config file saved in /tmp/sst2/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1924] 2021-04-20 13:00:04,900 >> Special tokens file saved in /tmp/sst2/checkpoint-500/special_tokens_map.json\n","{'loss': 0.2791, 'learning_rate': 1.6832937450514647e-05, 'epoch': 0.48}\n"," 16% 1000/6315 [12:05<1:04:47,  1.37it/s][INFO|trainer.py:1763] 2021-04-20 13:06:14,826 >> Saving model checkpoint to /tmp/sst2/checkpoint-1000\n","[INFO|configuration_utils.py:329] 2021-04-20 13:06:14,827 >> Configuration saved in /tmp/sst2/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:848] 2021-04-20 13:06:16,136 >> Model weights saved in /tmp/sst2/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1918] 2021-04-20 13:06:16,137 >> tokenizer config file saved in /tmp/sst2/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1924] 2021-04-20 13:06:16,137 >> Special tokens file saved in /tmp/sst2/checkpoint-1000/special_tokens_map.json\n","{'loss': 0.2326, 'learning_rate': 1.5249406175771972e-05, 'epoch': 0.71}\n"," 24% 1500/6315 [18:17<58:41,  1.37it/s][INFO|trainer.py:1763] 2021-04-20 13:12:26,118 >> Saving model checkpoint to /tmp/sst2/checkpoint-1500\n","[INFO|configuration_utils.py:329] 2021-04-20 13:12:26,119 >> Configuration saved in /tmp/sst2/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:848] 2021-04-20 13:12:27,581 >> Model weights saved in /tmp/sst2/checkpoint-1500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1918] 2021-04-20 13:12:27,581 >> tokenizer config file saved in /tmp/sst2/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1924] 2021-04-20 13:12:27,582 >> Special tokens file saved in /tmp/sst2/checkpoint-1500/special_tokens_map.json\n","{'loss': 0.2033, 'learning_rate': 1.3665874901029297e-05, 'epoch': 0.95}\n"," 32% 2000/6315 [24:27<52:36,  1.37it/s][INFO|trainer.py:1763] 2021-04-20 13:18:36,963 >> Saving model checkpoint to /tmp/sst2/checkpoint-2000\n","[INFO|configuration_utils.py:329] 2021-04-20 13:18:36,963 >> Configuration saved in /tmp/sst2/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:848] 2021-04-20 13:18:38,320 >> Model weights saved in /tmp/sst2/checkpoint-2000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1918] 2021-04-20 13:18:38,321 >> tokenizer config file saved in /tmp/sst2/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1924] 2021-04-20 13:18:38,321 >> Special tokens file saved in /tmp/sst2/checkpoint-2000/special_tokens_map.json\n","{'loss': 0.1513, 'learning_rate': 1.208234362628662e-05, 'epoch': 1.19}\n"," 40% 2500/6315 [30:38<46:23,  1.37it/s][INFO|trainer.py:1763] 2021-04-20 13:24:47,517 >> Saving model checkpoint to /tmp/sst2/checkpoint-2500\n","[INFO|configuration_utils.py:329] 2021-04-20 13:24:47,518 >> Configuration saved in /tmp/sst2/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:848] 2021-04-20 13:24:48,817 >> Model weights saved in /tmp/sst2/checkpoint-2500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1918] 2021-04-20 13:24:48,817 >> tokenizer config file saved in /tmp/sst2/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1924] 2021-04-20 13:24:48,818 >> Special tokens file saved in /tmp/sst2/checkpoint-2500/special_tokens_map.json\n","{'loss': 0.1399, 'learning_rate': 1.0498812351543943e-05, 'epoch': 1.43}\n"," 48% 3000/6315 [36:48<40:53,  1.35it/s][INFO|trainer.py:1763] 2021-04-20 13:30:57,941 >> Saving model checkpoint to /tmp/sst2/checkpoint-3000\n","[INFO|configuration_utils.py:329] 2021-04-20 13:30:57,942 >> Configuration saved in /tmp/sst2/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:848] 2021-04-20 13:30:59,359 >> Model weights saved in /tmp/sst2/checkpoint-3000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1918] 2021-04-20 13:30:59,360 >> tokenizer config file saved in /tmp/sst2/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1924] 2021-04-20 13:30:59,360 >> Special tokens file saved in /tmp/sst2/checkpoint-3000/special_tokens_map.json\n","{'loss': 0.1414, 'learning_rate': 8.915281076801267e-06, 'epoch': 1.66}\n"," 55% 3500/6315 [42:59<34:26,  1.36it/s][INFO|trainer.py:1763] 2021-04-20 13:37:08,527 >> Saving model checkpoint to /tmp/sst2/checkpoint-3500\n","[INFO|configuration_utils.py:329] 2021-04-20 13:37:08,528 >> Configuration saved in /tmp/sst2/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:848] 2021-04-20 13:37:09,902 >> Model weights saved in /tmp/sst2/checkpoint-3500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1918] 2021-04-20 13:37:09,903 >> tokenizer config file saved in /tmp/sst2/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1924] 2021-04-20 13:37:09,903 >> Special tokens file saved in /tmp/sst2/checkpoint-3500/special_tokens_map.json\n","{'loss': 0.1258, 'learning_rate': 7.331749802058591e-06, 'epoch': 1.9}\n"," 63% 4000/6315 [49:09<28:11,  1.37it/s][INFO|trainer.py:1763] 2021-04-20 13:43:18,557 >> Saving model checkpoint to /tmp/sst2/checkpoint-4000\n","[INFO|configuration_utils.py:329] 2021-04-20 13:43:18,558 >> Configuration saved in /tmp/sst2/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:848] 2021-04-20 13:43:19,767 >> Model weights saved in /tmp/sst2/checkpoint-4000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1918] 2021-04-20 13:43:19,768 >> tokenizer config file saved in /tmp/sst2/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1924] 2021-04-20 13:43:19,768 >> Special tokens file saved in /tmp/sst2/checkpoint-4000/special_tokens_map.json\n","{'loss': 0.1052, 'learning_rate': 5.748218527315916e-06, 'epoch': 2.14}\n"," 71% 4500/6315 [55:18<21:59,  1.38it/s][INFO|trainer.py:1763] 2021-04-20 13:49:27,692 >> Saving model checkpoint to /tmp/sst2/checkpoint-4500\n","[INFO|configuration_utils.py:329] 2021-04-20 13:49:27,693 >> Configuration saved in /tmp/sst2/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:848] 2021-04-20 13:49:28,980 >> Model weights saved in /tmp/sst2/checkpoint-4500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1918] 2021-04-20 13:49:28,981 >> tokenizer config file saved in /tmp/sst2/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1924] 2021-04-20 13:49:28,981 >> Special tokens file saved in /tmp/sst2/checkpoint-4500/special_tokens_map.json\n","{'loss': 0.0942, 'learning_rate': 4.164687252573238e-06, 'epoch': 2.38}\n"," 79% 5000/6315 [1:01:27<15:55,  1.38it/s][INFO|trainer.py:1763] 2021-04-20 13:55:36,288 >> Saving model checkpoint to /tmp/sst2/checkpoint-5000\n","[INFO|configuration_utils.py:329] 2021-04-20 13:55:36,289 >> Configuration saved in /tmp/sst2/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:848] 2021-04-20 13:55:37,491 >> Model weights saved in /tmp/sst2/checkpoint-5000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1918] 2021-04-20 13:55:37,492 >> tokenizer config file saved in /tmp/sst2/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1924] 2021-04-20 13:55:37,492 >> Special tokens file saved in /tmp/sst2/checkpoint-5000/special_tokens_map.json\n","{'loss': 0.091, 'learning_rate': 2.581155977830562e-06, 'epoch': 2.61}\n"," 87% 5500/6315 [1:07:35<09:52,  1.37it/s][INFO|trainer.py:1763] 2021-04-20 14:01:45,034 >> Saving model checkpoint to /tmp/sst2/checkpoint-5500\n","[INFO|configuration_utils.py:329] 2021-04-20 14:01:45,035 >> Configuration saved in /tmp/sst2/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:848] 2021-04-20 14:01:46,387 >> Model weights saved in /tmp/sst2/checkpoint-5500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1918] 2021-04-20 14:01:46,388 >> tokenizer config file saved in /tmp/sst2/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1924] 2021-04-20 14:01:46,388 >> Special tokens file saved in /tmp/sst2/checkpoint-5500/special_tokens_map.json\n","{'loss': 0.0835, 'learning_rate': 9.976247030878861e-07, 'epoch': 2.85}\n"," 95% 6000/6315 [1:13:44<03:50,  1.37it/s][INFO|trainer.py:1763] 2021-04-20 14:07:53,884 >> Saving model checkpoint to /tmp/sst2/checkpoint-6000\n","[INFO|configuration_utils.py:329] 2021-04-20 14:07:53,885 >> Configuration saved in /tmp/sst2/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:848] 2021-04-20 14:07:55,039 >> Model weights saved in /tmp/sst2/checkpoint-6000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1918] 2021-04-20 14:07:55,039 >> tokenizer config file saved in /tmp/sst2/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1924] 2021-04-20 14:07:55,039 >> Special tokens file saved in /tmp/sst2/checkpoint-6000/special_tokens_map.json\n","100% 6315/6315 [1:17:38<00:00,  1.49it/s][INFO|trainer.py:1297] 2021-04-20 14:11:47,756 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 4658.6896, 'train_samples_per_second': 1.356, 'epoch': 3.0}\n","100% 6315/6315 [1:17:38<00:00,  1.36it/s]\n","[INFO|trainer.py:1763] 2021-04-20 14:11:48,013 >> Saving model checkpoint to /tmp/sst2/\n","[INFO|configuration_utils.py:329] 2021-04-20 14:11:48,013 >> Configuration saved in /tmp/sst2/config.json\n","[INFO|modeling_utils.py:848] 2021-04-20 14:11:49,015 >> Model weights saved in /tmp/sst2/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1918] 2021-04-20 14:11:49,016 >> tokenizer config file saved in /tmp/sst2/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1924] 2021-04-20 14:11:49,016 >> Special tokens file saved in /tmp/sst2/special_tokens_map.json\n","[INFO|trainer_pt_utils.py:898] 2021-04-20 14:11:49,049 >> ***** train metrics *****\n","[INFO|trainer_pt_utils.py:903] 2021-04-20 14:11:49,115 >>   epoch                      =        3.0\n","[INFO|trainer_pt_utils.py:903] 2021-04-20 14:11:49,115 >>   init_mem_cpu_alloc_delta   =     2257MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-20 14:11:49,115 >>   init_mem_cpu_peaked_delta  =        0MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-20 14:11:49,115 >>   init_mem_gpu_alloc_delta   =      413MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-20 14:11:49,115 >>   init_mem_gpu_peaked_delta  =        0MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-20 14:11:49,115 >>   train_mem_cpu_alloc_delta  =     -213MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-20 14:11:49,115 >>   train_mem_cpu_peaked_delta =      278MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-20 14:11:49,115 >>   train_mem_gpu_alloc_delta  =     1298MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-20 14:11:49,115 >>   train_mem_gpu_peaked_delta =     3395MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-20 14:11:49,115 >>   train_runtime              = 1:17:38.68\n","[INFO|trainer_pt_utils.py:903] 2021-04-20 14:11:49,115 >>   train_samples              =      67349\n","[INFO|trainer_pt_utils.py:903] 2021-04-20 14:11:49,115 >>   train_samples_per_second   =      1.356\n","04/20/2021 14:11:49 - INFO - __main__ -   *** Evaluate ***\n","[INFO|trainer.py:502] 2021-04-20 14:11:49,264 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence.\n","[INFO|trainer.py:1983] 2021-04-20 14:11:49,265 >> ***** Running Evaluation *****\n","[INFO|trainer.py:1985] 2021-04-20 14:11:49,265 >>   Num examples = 872\n","[INFO|trainer.py:1988] 2021-04-20 14:11:49,266 >>   Batch size = 8\n","100% 109/109 [00:07<00:00, 15.21it/s]\n","[INFO|trainer_pt_utils.py:898] 2021-04-20 14:11:56,607 >> ***** eval metrics *****\n","[INFO|trainer_pt_utils.py:903] 2021-04-20 14:11:56,607 >>   epoch                     =        3.0\n","[INFO|trainer_pt_utils.py:903] 2021-04-20 14:11:56,607 >>   eval_accuracy             =     0.8968\n","[INFO|trainer_pt_utils.py:903] 2021-04-20 14:11:56,607 >>   eval_loss                 =     0.3785\n","[INFO|trainer_pt_utils.py:903] 2021-04-20 14:11:56,607 >>   eval_mem_cpu_alloc_delta  =        3MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-20 14:11:56,607 >>   eval_mem_cpu_peaked_delta =        0MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-20 14:11:56,607 >>   eval_mem_gpu_alloc_delta  =        0MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-20 14:11:56,607 >>   eval_mem_gpu_peaked_delta =       33MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-20 14:11:56,607 >>   eval_runtime              = 0:00:07.20\n","[INFO|trainer_pt_utils.py:903] 2021-04-20 14:11:56,607 >>   eval_samples              =        872\n","[INFO|trainer_pt_utils.py:903] 2021-04-20 14:11:56,607 >>   eval_samples_per_second   =    121.018\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rSIoPy0Uo20Q","outputId":"940aef99-0dde-4a04-ca59-93af23388a3a"},"source":["!python run_glue.py \\\n","  --model_name_or_path dmis-lab/biobert-base-cased-v1.1 \\\n","  --task_name stsb \\\n","  --do_train \\\n","  --do_eval \\\n","  --max_seq_length 128 \\\n","  --per_device_train_batch_size 32 \\\n","  --learning_rate 2e-5 \\\n","  --num_train_epochs 3 \\\n","  --output_dir /tmp/stsb/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-04-20 14:16:31.102905: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n","04/20/2021 14:16:34 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","04/20/2021 14:16:34 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/tmp/stsb/, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Apr20_14-16-34_873cfbb4fb8a, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/tmp/stsb/, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, _n_gpu=1, mp_parameters=)\n","Downloading and preparing dataset glue/stsb (download: 784.05 KiB, generated: 1.09 MiB, post-processed: Unknown size, total: 1.86 MiB) to /root/.cache/huggingface/datasets/glue/stsb/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4...\n","Downloading: 100% 803k/803k [00:00<00:00, 1.69MB/s]\n","Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/stsb/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4. Subsequent calls will reuse this data.\n","[INFO|configuration_utils.py:491] 2021-04-20 14:16:36,426 >> loading configuration file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/78e6e8ece5b58501028ce314273009ad7707ef4c5ba44251914fd6bca8a05eff.e4a2e693122d98b8b56b7dc1f0d89b644226aacef228afb5030ee3621b2829d3\n","[INFO|configuration_utils.py:527] 2021-04-20 14:16:36,426 >> Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": \"stsb\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.6.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|configuration_utils.py:491] 2021-04-20 14:16:36,442 >> loading configuration file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/78e6e8ece5b58501028ce314273009ad7707ef4c5ba44251914fd6bca8a05eff.e4a2e693122d98b8b56b7dc1f0d89b644226aacef228afb5030ee3621b2829d3\n","[INFO|configuration_utils.py:527] 2021-04-20 14:16:36,442 >> Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.6.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|tokenization_utils_base.py:1713] 2021-04-20 14:16:36,522 >> loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/cda9c238619f77c4dbf65f23ab841b5239f0289800d673957c843b9dda52ad44.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|tokenization_utils_base.py:1713] 2021-04-20 14:16:36,522 >> loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:1713] 2021-04-20 14:16:36,522 >> loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1713] 2021-04-20 14:16:36,523 >> loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1713] 2021-04-20 14:16:36,523 >> loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/tokenizer_config.json from cache at None\n","[INFO|modeling_utils.py:1075] 2021-04-20 14:16:36,578 >> loading weights file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/f22d3e86464eb4aa2e5f76e0560e9d595617f55e014c78f176b834f9245a62f0.2cf22e1fb1f2db88b971c8da5593b1e94a0ee79d8c892a0f6d4f607d2b77fa23\n","[WARNING|modeling_utils.py:1196] 2021-04-20 14:16:51,647 >> Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.1 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:1207] 2021-04-20 14:16:51,647 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","100% 6/6 [00:00<00:00, 10.39ba/s]\n","100% 2/2 [00:00<00:00, 13.07ba/s]\n","100% 2/2 [00:00<00:00, 14.79ba/s]\n","04/20/2021 14:16:52 - INFO - __main__ -   Sample 5238 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 5238, 'input_ids': [101, 1225, 2852, 1231, 5730, 10097, 1113, 188, 12577, 1465, 102, 184, 2822, 1918, 112, 188, 1285, 131, 5748, 1159, 1113, 188, 12577, 1465, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1.600000023841858, 'sentence1': 'Didier Reynders on Syria', 'sentence2': \"Obama's day: Prime time on Syria\", 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n","04/20/2021 14:16:52 - INFO - __main__ -   Sample 912 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 912, 'input_ids': [101, 170, 1299, 1110, 9374, 1103, 3751, 119, 102, 170, 1299, 1110, 3759, 170, 1610, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 0.4000000059604645, 'sentence1': 'A man is cleaning the windows.', 'sentence2': 'A man is driving a car.', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n","04/20/2021 14:16:52 - INFO - __main__ -   Sample 204 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 204, 'input_ids': [101, 170, 1590, 3486, 170, 24181, 12253, 24886, 119, 102, 170, 1590, 1110, 8184, 1146, 170, 24181, 12253, 24886, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 3.25, 'sentence1': 'A woman holds a kangaroo.', 'sentence2': 'A woman is picking up a kangaroo.', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n","[INFO|trainer.py:502] 2021-04-20 14:16:57,376 >> The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.\n","[INFO|trainer.py:502] 2021-04-20 14:16:57,377 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx.\n","[INFO|trainer.py:1102] 2021-04-20 14:16:57,553 >> ***** Running training *****\n","[INFO|trainer.py:1103] 2021-04-20 14:16:57,553 >>   Num examples = 5749\n","[INFO|trainer.py:1104] 2021-04-20 14:16:57,553 >>   Num Epochs = 3\n","[INFO|trainer.py:1105] 2021-04-20 14:16:57,553 >>   Instantaneous batch size per device = 32\n","[INFO|trainer.py:1106] 2021-04-20 14:16:57,553 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n","[INFO|trainer.py:1107] 2021-04-20 14:16:57,553 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1108] 2021-04-20 14:16:57,553 >>   Total optimization steps = 540\n"," 14% 74/540 [00:51<05:33,  1.40it/s]Traceback (most recent call last):\n","  File \"run_glue.py\", line 527, in <module>\n","    main()\n","  File \"run_glue.py\", line 459, in main\n","    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n","  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 1217, in train\n","    tr_loss += self.training_step(model, inputs)\n","  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 1630, in training_step\n","    loss.backward()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/tensor.py\", line 245, in backward\n","    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 147, in backward\n","    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n","KeyboardInterrupt\n"," 14% 74/540 [00:52<05:27,  1.42it/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KU6hA6ytCh2i","outputId":"4eddaa1e-6376-49b6-f0c1-3a22b9f58c0d"},"source":["!python run_glue.py \\\n","  --model_name_or_path dmis-lab/biobert-base-cased-v1.1 \\\n","  --task_name qqp \\\n","  --do_train \\\n","  --do_eval \\\n","  --max_seq_length 128 \\\n","  --per_device_train_batch_size 32 \\\n","  --learning_rate 2e-5 \\\n","  --num_train_epochs 3 \\\n","  --output_dir /tmp/qqp/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-04-20 01:22:04.346796: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n","04/20/2021 01:22:06 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","04/20/2021 01:22:06 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/tmp/qqp/, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Apr20_01-22-06_c13a78669f38, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/tmp/qqp/, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, _n_gpu=1, mp_parameters=)\n","Downloading: 28.7kB [00:00, 17.6MB/s]       \n","Downloading: 28.7kB [00:00, 31.0MB/s]       \n","Downloading and preparing dataset glue/qqp (download: 39.76 MiB, generated: 106.55 MiB, post-processed: Unknown size, total: 146.32 MiB) to /root/.cache/huggingface/datasets/glue/qqp/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4...\n","Downloading: 100% 41.7M/41.7M [00:00<00:00, 51.1MB/s]\n","Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/qqp/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4. Subsequent calls will reuse this data.\n","[INFO|file_utils.py:1394] 2021-04-20 01:22:31,799 >> https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmptzmeevwo\n","Downloading: 100% 313/313 [00:00<00:00, 342kB/s]\n","[INFO|file_utils.py:1398] 2021-04-20 01:22:32,002 >> storing https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/78e6e8ece5b58501028ce314273009ad7707ef4c5ba44251914fd6bca8a05eff.e4a2e693122d98b8b56b7dc1f0d89b644226aacef228afb5030ee3621b2829d3\n","[INFO|file_utils.py:1401] 2021-04-20 01:22:32,002 >> creating metadata file for /root/.cache/huggingface/transformers/78e6e8ece5b58501028ce314273009ad7707ef4c5ba44251914fd6bca8a05eff.e4a2e693122d98b8b56b7dc1f0d89b644226aacef228afb5030ee3621b2829d3\n","[INFO|configuration_utils.py:491] 2021-04-20 01:22:32,002 >> loading configuration file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/78e6e8ece5b58501028ce314273009ad7707ef4c5ba44251914fd6bca8a05eff.e4a2e693122d98b8b56b7dc1f0d89b644226aacef228afb5030ee3621b2829d3\n","[INFO|configuration_utils.py:527] 2021-04-20 01:22:32,003 >> Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": \"qqp\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.6.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|configuration_utils.py:491] 2021-04-20 01:22:32,208 >> loading configuration file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/78e6e8ece5b58501028ce314273009ad7707ef4c5ba44251914fd6bca8a05eff.e4a2e693122d98b8b56b7dc1f0d89b644226aacef228afb5030ee3621b2829d3\n","[INFO|configuration_utils.py:527] 2021-04-20 01:22:32,208 >> Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.6.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|file_utils.py:1394] 2021-04-20 01:22:32,425 >> https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpperlenqq\n","Downloading: 100% 213k/213k [00:00<00:00, 845kB/s]\n","[INFO|file_utils.py:1398] 2021-04-20 01:22:32,884 >> storing https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/cda9c238619f77c4dbf65f23ab841b5239f0289800d673957c843b9dda52ad44.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|file_utils.py:1401] 2021-04-20 01:22:32,884 >> creating metadata file for /root/.cache/huggingface/transformers/cda9c238619f77c4dbf65f23ab841b5239f0289800d673957c843b9dda52ad44.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|tokenization_utils_base.py:1713] 2021-04-20 01:22:33,709 >> loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/cda9c238619f77c4dbf65f23ab841b5239f0289800d673957c843b9dda52ad44.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|tokenization_utils_base.py:1713] 2021-04-20 01:22:33,710 >> loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:1713] 2021-04-20 01:22:33,710 >> loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1713] 2021-04-20 01:22:33,710 >> loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1713] 2021-04-20 01:22:33,710 >> loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/tokenizer_config.json from cache at None\n","[INFO|file_utils.py:1394] 2021-04-20 01:22:33,953 >> https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpguo6ojdv\n","Downloading: 100% 436M/436M [00:08<00:00, 52.7MB/s]\n","[INFO|file_utils.py:1398] 2021-04-20 01:22:42,305 >> storing https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/f22d3e86464eb4aa2e5f76e0560e9d595617f55e014c78f176b834f9245a62f0.2cf22e1fb1f2db88b971c8da5593b1e94a0ee79d8c892a0f6d4f607d2b77fa23\n","[INFO|file_utils.py:1401] 2021-04-20 01:22:42,306 >> creating metadata file for /root/.cache/huggingface/transformers/f22d3e86464eb4aa2e5f76e0560e9d595617f55e014c78f176b834f9245a62f0.2cf22e1fb1f2db88b971c8da5593b1e94a0ee79d8c892a0f6d4f607d2b77fa23\n","[INFO|modeling_utils.py:1075] 2021-04-20 01:22:42,306 >> loading weights file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/f22d3e86464eb4aa2e5f76e0560e9d595617f55e014c78f176b834f9245a62f0.2cf22e1fb1f2db88b971c8da5593b1e94a0ee79d8c892a0f6d4f607d2b77fa23\n","[WARNING|modeling_utils.py:1196] 2021-04-20 01:22:45,682 >> Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.1 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:1207] 2021-04-20 01:22:45,682 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","100% 364/364 [00:36<00:00,  9.99ba/s]\n","100% 41/41 [00:03<00:00, 10.32ba/s]\n","100% 391/391 [00:39<00:00,  9.97ba/s]\n","04/20/2021 01:24:05 - INFO - __main__ -   Sample 335243 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 335243, 'input_ids': [101, 1293, 1202, 178, 8672, 179, 3051, 1107, 170, 2370, 136, 102, 1293, 1202, 178, 8672, 179, 3051, 1107, 125, 118, 126, 1808, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 0, 'question1': 'How do I crack JEE in a month?', 'question2': 'How do I crack JEE in 4-5 months?', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n","04/20/2021 01:24:05 - INFO - __main__ -   Sample 58369 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 58369, 'input_ids': [101, 1150, 1132, 1103, 4459, 1234, 1107, 1103, 1362, 136, 102, 1169, 1128, 1271, 1199, 1234, 1150, 1138, 1541, 4987, 1103, 1362, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 0, 'question1': 'Who are the greatest people in the world?', 'question2': 'Can you name some people who have really saved the world?', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n","04/20/2021 01:24:05 - INFO - __main__ -   Sample 13112 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 13112, 'input_ids': [101, 1184, 1110, 1656, 170, 1338, 1233, 11321, 9983, 136, 102, 1132, 1338, 1233, 11321, 16595, 2011, 1106, 9781, 13964, 5427, 1733, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 0, 'question1': 'What is inside a Camel Crush cigarette?', 'question2': 'Are Camel Crush cigarettes designed to attract teen smokers?', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n","Downloading: 5.75kB [00:00, 5.61MB/s]       \n","[INFO|trainer.py:502] 2021-04-20 01:24:17,294 >> The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question2, idx, question1.\n","[INFO|trainer.py:502] 2021-04-20 01:24:17,295 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question2, idx, question1.\n","[INFO|trainer.py:1102] 2021-04-20 01:24:17,487 >> ***** Running training *****\n","[INFO|trainer.py:1103] 2021-04-20 01:24:17,487 >>   Num examples = 363846\n","[INFO|trainer.py:1104] 2021-04-20 01:24:17,487 >>   Num Epochs = 3\n","[INFO|trainer.py:1105] 2021-04-20 01:24:17,488 >>   Instantaneous batch size per device = 32\n","[INFO|trainer.py:1106] 2021-04-20 01:24:17,488 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n","[INFO|trainer.py:1107] 2021-04-20 01:24:17,488 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1108] 2021-04-20 01:24:17,488 >>   Total optimization steps = 34113\n","{'loss': 0.4614, 'learning_rate': 1.9706856623574592e-05, 'epoch': 0.04}\n","  1% 500/34113 [05:57<6:49:48,  1.37it/s][INFO|trainer.py:1763] 2021-04-20 01:30:14,830 >> Saving model checkpoint to /tmp/qqp/checkpoint-500\n","[INFO|configuration_utils.py:329] 2021-04-20 01:30:14,831 >> Configuration saved in /tmp/qqp/checkpoint-500/config.json\n","[INFO|modeling_utils.py:848] 2021-04-20 01:30:15,941 >> Model weights saved in /tmp/qqp/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1918] 2021-04-20 01:30:15,942 >> tokenizer config file saved in /tmp/qqp/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1924] 2021-04-20 01:30:15,942 >> Special tokens file saved in /tmp/qqp/checkpoint-500/special_tokens_map.json\n","{'loss': 0.3784, 'learning_rate': 1.9413713247149183e-05, 'epoch': 0.09}\n","  3% 1000/34113 [12:05<6:42:14,  1.37it/s][INFO|trainer.py:1763] 2021-04-20 01:36:23,362 >> Saving model checkpoint to /tmp/qqp/checkpoint-1000\n","[INFO|configuration_utils.py:329] 2021-04-20 01:36:23,363 >> Configuration saved in /tmp/qqp/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:848] 2021-04-20 01:36:24,569 >> Model weights saved in /tmp/qqp/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1918] 2021-04-20 01:36:24,570 >> tokenizer config file saved in /tmp/qqp/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1924] 2021-04-20 01:36:24,570 >> Special tokens file saved in /tmp/qqp/checkpoint-1000/special_tokens_map.json\n","{'loss': 0.3566, 'learning_rate': 1.9120569870723774e-05, 'epoch': 0.13}\n","  4% 1500/34113 [18:14<6:35:11,  1.38it/s][INFO|trainer.py:1763] 2021-04-20 01:42:31,711 >> Saving model checkpoint to /tmp/qqp/checkpoint-1500\n","[INFO|configuration_utils.py:329] 2021-04-20 01:42:31,711 >> Configuration saved in /tmp/qqp/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:848] 2021-04-20 01:42:32,954 >> Model weights saved in /tmp/qqp/checkpoint-1500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1918] 2021-04-20 01:42:32,955 >> tokenizer config file saved in /tmp/qqp/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1924] 2021-04-20 01:42:32,955 >> Special tokens file saved in /tmp/qqp/checkpoint-1500/special_tokens_map.json\n","{'loss': 0.3318, 'learning_rate': 1.882742649429836e-05, 'epoch': 0.18}\n","  6% 2000/34113 [24:22<6:29:32,  1.37it/s][INFO|trainer.py:1763] 2021-04-20 01:48:40,229 >> Saving model checkpoint to /tmp/qqp/checkpoint-2000\n","[INFO|configuration_utils.py:329] 2021-04-20 01:48:40,230 >> Configuration saved in /tmp/qqp/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:848] 2021-04-20 01:48:41,570 >> Model weights saved in /tmp/qqp/checkpoint-2000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1918] 2021-04-20 01:48:41,571 >> tokenizer config file saved in /tmp/qqp/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1924] 2021-04-20 01:48:41,571 >> Special tokens file saved in /tmp/qqp/checkpoint-2000/special_tokens_map.json\n","{'loss': 0.3222, 'learning_rate': 1.8534283117872952e-05, 'epoch': 0.22}\n","  7% 2500/34113 [30:31<6:26:21,  1.36it/s][INFO|trainer.py:1763] 2021-04-20 01:54:48,866 >> Saving model checkpoint to /tmp/qqp/checkpoint-2500\n","[INFO|configuration_utils.py:329] 2021-04-20 01:54:48,867 >> Configuration saved in /tmp/qqp/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:848] 2021-04-20 01:54:50,106 >> Model weights saved in /tmp/qqp/checkpoint-2500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1918] 2021-04-20 01:54:50,106 >> tokenizer config file saved in /tmp/qqp/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1924] 2021-04-20 01:54:50,107 >> Special tokens file saved in /tmp/qqp/checkpoint-2500/special_tokens_map.json\n","{'loss': 0.3113, 'learning_rate': 1.8241139741447543e-05, 'epoch': 0.26}\n","  9% 3000/34113 [36:43<6:17:35,  1.37it/s][INFO|trainer.py:1763] 2021-04-20 02:01:00,740 >> Saving model checkpoint to /tmp/qqp/checkpoint-3000\n","[INFO|configuration_utils.py:329] 2021-04-20 02:01:00,741 >> Configuration saved in /tmp/qqp/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:848] 2021-04-20 02:01:01,959 >> Model weights saved in /tmp/qqp/checkpoint-3000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1918] 2021-04-20 02:01:01,959 >> tokenizer config file saved in /tmp/qqp/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1924] 2021-04-20 02:01:01,960 >> Special tokens file saved in /tmp/qqp/checkpoint-3000/special_tokens_map.json\n","{'loss': 0.3076, 'learning_rate': 1.7947996365022134e-05, 'epoch': 0.31}\n"," 10% 3500/34113 [42:51<6:11:02,  1.38it/s][INFO|trainer.py:1763] 2021-04-20 02:07:08,942 >> Saving model checkpoint to /tmp/qqp/checkpoint-3500\n","[INFO|configuration_utils.py:329] 2021-04-20 02:07:08,943 >> Configuration saved in /tmp/qqp/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:848] 2021-04-20 02:07:10,347 >> Model weights saved in /tmp/qqp/checkpoint-3500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1918] 2021-04-20 02:07:10,347 >> tokenizer config file saved in /tmp/qqp/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1924] 2021-04-20 02:07:10,348 >> Special tokens file saved in /tmp/qqp/checkpoint-3500/special_tokens_map.json\n","{'loss': 0.3006, 'learning_rate': 1.7654852988596725e-05, 'epoch': 0.35}\n"," 12% 4000/34113 [48:59<6:05:21,  1.37it/s][INFO|trainer.py:1763] 2021-04-20 02:13:17,231 >> Saving model checkpoint to /tmp/qqp/checkpoint-4000\n","[INFO|configuration_utils.py:329] 2021-04-20 02:13:17,232 >> Configuration saved in /tmp/qqp/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:848] 2021-04-20 02:13:18,531 >> Model weights saved in /tmp/qqp/checkpoint-4000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1918] 2021-04-20 02:13:18,531 >> tokenizer config file saved in /tmp/qqp/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1924] 2021-04-20 02:13:18,532 >> Special tokens file saved in /tmp/qqp/checkpoint-4000/special_tokens_map.json\n","{'loss': 0.2964, 'learning_rate': 1.7361709612171312e-05, 'epoch': 0.4}\n"," 13% 4500/34113 [55:07<5:59:08,  1.37it/s][INFO|trainer.py:1763] 2021-04-20 02:19:25,109 >> Saving model checkpoint to /tmp/qqp/checkpoint-4500\n","[INFO|configuration_utils.py:329] 2021-04-20 02:19:25,110 >> Configuration saved in /tmp/qqp/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:848] 2021-04-20 02:19:26,428 >> Model weights saved in /tmp/qqp/checkpoint-4500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1918] 2021-04-20 02:19:26,430 >> tokenizer config file saved in /tmp/qqp/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1924] 2021-04-20 02:19:26,430 >> Special tokens file saved in /tmp/qqp/checkpoint-4500/special_tokens_map.json\n","{'loss': 0.2921, 'learning_rate': 1.7068566235745903e-05, 'epoch': 0.44}\n"," 15% 5000/34113 [1:01:15<5:52:44,  1.38it/s][INFO|trainer.py:1763] 2021-04-20 02:25:33,183 >> Saving model checkpoint to /tmp/qqp/checkpoint-5000\n","[INFO|configuration_utils.py:329] 2021-04-20 02:25:33,184 >> Configuration saved in /tmp/qqp/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:848] 2021-04-20 02:25:34,397 >> Model weights saved in /tmp/qqp/checkpoint-5000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1918] 2021-04-20 02:25:34,398 >> tokenizer config file saved in /tmp/qqp/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1924] 2021-04-20 02:25:34,398 >> Special tokens file saved in /tmp/qqp/checkpoint-5000/special_tokens_map.json\n","{'loss': 0.2836, 'learning_rate': 1.6775422859320497e-05, 'epoch': 0.48}\n"," 16% 5500/34113 [1:07:23<5:45:55,  1.38it/s][INFO|trainer.py:1763] 2021-04-20 02:31:41,019 >> Saving model checkpoint to /tmp/qqp/checkpoint-5500\n","[INFO|configuration_utils.py:329] 2021-04-20 02:31:41,020 >> Configuration saved in /tmp/qqp/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:848] 2021-04-20 02:31:42,448 >> Model weights saved in /tmp/qqp/checkpoint-5500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1918] 2021-04-20 02:31:42,449 >> tokenizer config file saved in /tmp/qqp/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1924] 2021-04-20 02:31:42,449 >> Special tokens file saved in /tmp/qqp/checkpoint-5500/special_tokens_map.json\n","{'loss': 0.2817, 'learning_rate': 1.6482279482895085e-05, 'epoch': 0.53}\n"," 18% 6000/34113 [1:13:31<5:40:53,  1.37it/s][INFO|trainer.py:1763] 2021-04-20 02:37:49,114 >> Saving model checkpoint to /tmp/qqp/checkpoint-6000\n","[INFO|configuration_utils.py:329] 2021-04-20 02:37:49,115 >> Configuration saved in /tmp/qqp/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:848] 2021-04-20 02:37:50,359 >> Model weights saved in /tmp/qqp/checkpoint-6000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1918] 2021-04-20 02:37:50,360 >> tokenizer config file saved in /tmp/qqp/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1924] 2021-04-20 02:37:50,360 >> Special tokens file saved in /tmp/qqp/checkpoint-6000/special_tokens_map.json\n","{'loss': 0.2867, 'learning_rate': 1.6189136106469676e-05, 'epoch': 0.57}\n"," 19% 6500/34113 [1:19:39<5:35:03,  1.37it/s][INFO|trainer.py:1763] 2021-04-20 02:43:57,055 >> Saving model checkpoint to /tmp/qqp/checkpoint-6500\n","[INFO|configuration_utils.py:329] 2021-04-20 02:43:57,056 >> Configuration saved in /tmp/qqp/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:848] 2021-04-20 02:43:58,200 >> Model weights saved in /tmp/qqp/checkpoint-6500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1918] 2021-04-20 02:43:58,201 >> tokenizer config file saved in /tmp/qqp/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1924] 2021-04-20 02:43:58,202 >> Special tokens file saved in /tmp/qqp/checkpoint-6500/special_tokens_map.json\n","{'loss': 0.2785, 'learning_rate': 1.5895992730044266e-05, 'epoch': 0.62}\n"," 21% 7000/34113 [1:25:48<5:29:49,  1.37it/s][INFO|trainer.py:1763] 2021-04-20 02:50:05,675 >> Saving model checkpoint to /tmp/qqp/checkpoint-7000\n","[INFO|configuration_utils.py:329] 2021-04-20 02:50:05,676 >> Configuration saved in /tmp/qqp/checkpoint-7000/config.json\n","[INFO|modeling_utils.py:848] 2021-04-20 02:50:06,924 >> Model weights saved in /tmp/qqp/checkpoint-7000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1918] 2021-04-20 02:50:06,924 >> tokenizer config file saved in /tmp/qqp/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1924] 2021-04-20 02:50:06,924 >> Special tokens file saved in /tmp/qqp/checkpoint-7000/special_tokens_map.json\n","{'loss': 0.2719, 'learning_rate': 1.5602849353618857e-05, 'epoch': 0.66}\n"," 22% 7500/34113 [1:31:56<5:22:30,  1.38it/s][INFO|trainer.py:1763] 2021-04-20 02:56:14,105 >> Saving model checkpoint to /tmp/qqp/checkpoint-7500\n","[INFO|configuration_utils.py:329] 2021-04-20 02:56:14,106 >> Configuration saved in /tmp/qqp/checkpoint-7500/config.json\n","[INFO|modeling_utils.py:848] 2021-04-20 02:56:15,287 >> Model weights saved in /tmp/qqp/checkpoint-7500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1918] 2021-04-20 02:56:15,288 >> tokenizer config file saved in /tmp/qqp/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1924] 2021-04-20 02:56:15,289 >> Special tokens file saved in /tmp/qqp/checkpoint-7500/special_tokens_map.json\n"," 23% 7947/34113 [1:37:26<5:17:50,  1.37it/s]"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gIAOZic5gSil"},"source":[""]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BmKM13s22ws1","outputId":"cf1d153c-7993-4d61-ba5e-9714c4a16697"},"source":["!python run_glue.py \\\n","  --model_name_or_path dmis-lab/biobert-base-cased-v1.1 \\\n","  --task_name mrpc \\\n","  --do_train \\\n","  --do_eval \\\n","  --max_seq_length 128 \\\n","  --per_device_train_batch_size 32 \\\n","  --learning_rate 2e-5 \\\n","  --num_train_epochs 3 \\\n","  --output_dir /tmp/mrpc/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-04-19 21:17:38.251421: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n","04/19/2021 21:17:40 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","04/19/2021 21:17:40 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/tmp/mrpc/, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Apr19_21-17-40_3483064768e1, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/tmp/mrpc/, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, _n_gpu=1, mp_parameters=)\n","Downloading: 28.7kB [00:00, 23.1MB/s]       \n","Downloading: 28.7kB [00:00, 25.7MB/s]       \n","Downloading and preparing dataset glue/mrpc (download: 1.43 MiB, generated: 1.43 MiB, post-processed: Unknown size, total: 2.85 MiB) to /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4...\n","Downloading: 6.22kB [00:00, 4.40MB/s]\n","Downloading: 1.05MB [00:00, 7.02MB/s]\n","Downloading: 441kB [00:00, 4.13MB/s]\n","Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4. Subsequent calls will reuse this data.\n","[INFO|file_utils.py:1394] 2021-04-19 21:17:43,939 >> https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpfo9a56dc\n","Downloading: 100% 313/313 [00:00<00:00, 308kB/s]\n","[INFO|file_utils.py:1398] 2021-04-19 21:17:44,296 >> storing https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/78e6e8ece5b58501028ce314273009ad7707ef4c5ba44251914fd6bca8a05eff.e4a2e693122d98b8b56b7dc1f0d89b644226aacef228afb5030ee3621b2829d3\n","[INFO|file_utils.py:1401] 2021-04-19 21:17:44,296 >> creating metadata file for /root/.cache/huggingface/transformers/78e6e8ece5b58501028ce314273009ad7707ef4c5ba44251914fd6bca8a05eff.e4a2e693122d98b8b56b7dc1f0d89b644226aacef228afb5030ee3621b2829d3\n","[INFO|configuration_utils.py:491] 2021-04-19 21:17:44,296 >> loading configuration file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/78e6e8ece5b58501028ce314273009ad7707ef4c5ba44251914fd6bca8a05eff.e4a2e693122d98b8b56b7dc1f0d89b644226aacef228afb5030ee3621b2829d3\n","[INFO|configuration_utils.py:527] 2021-04-19 21:17:44,297 >> Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": \"mrpc\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.6.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|configuration_utils.py:491] 2021-04-19 21:17:44,500 >> loading configuration file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/78e6e8ece5b58501028ce314273009ad7707ef4c5ba44251914fd6bca8a05eff.e4a2e693122d98b8b56b7dc1f0d89b644226aacef228afb5030ee3621b2829d3\n","[INFO|configuration_utils.py:527] 2021-04-19 21:17:44,500 >> Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.6.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|file_utils.py:1394] 2021-04-19 21:17:44,707 >> https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpauh6o4ow\n","Downloading: 100% 213k/213k [00:00<00:00, 850kB/s]\n","[INFO|file_utils.py:1398] 2021-04-19 21:17:45,163 >> storing https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/cda9c238619f77c4dbf65f23ab841b5239f0289800d673957c843b9dda52ad44.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|file_utils.py:1401] 2021-04-19 21:17:45,163 >> creating metadata file for /root/.cache/huggingface/transformers/cda9c238619f77c4dbf65f23ab841b5239f0289800d673957c843b9dda52ad44.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|tokenization_utils_base.py:1713] 2021-04-19 21:17:45,985 >> loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/cda9c238619f77c4dbf65f23ab841b5239f0289800d673957c843b9dda52ad44.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|tokenization_utils_base.py:1713] 2021-04-19 21:17:45,985 >> loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:1713] 2021-04-19 21:17:45,986 >> loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1713] 2021-04-19 21:17:45,986 >> loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1713] 2021-04-19 21:17:45,986 >> loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/tokenizer_config.json from cache at None\n","[INFO|file_utils.py:1394] 2021-04-19 21:17:46,238 >> https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp_q3h0bc6\n","Downloading: 100% 436M/436M [00:06<00:00, 67.0MB/s]\n","[INFO|file_utils.py:1398] 2021-04-19 21:17:52,793 >> storing https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/f22d3e86464eb4aa2e5f76e0560e9d595617f55e014c78f176b834f9245a62f0.2cf22e1fb1f2db88b971c8da5593b1e94a0ee79d8c892a0f6d4f607d2b77fa23\n","[INFO|file_utils.py:1401] 2021-04-19 21:17:52,793 >> creating metadata file for /root/.cache/huggingface/transformers/f22d3e86464eb4aa2e5f76e0560e9d595617f55e014c78f176b834f9245a62f0.2cf22e1fb1f2db88b971c8da5593b1e94a0ee79d8c892a0f6d4f607d2b77fa23\n","[INFO|modeling_utils.py:1075] 2021-04-19 21:17:52,794 >> loading weights file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/f22d3e86464eb4aa2e5f76e0560e9d595617f55e014c78f176b834f9245a62f0.2cf22e1fb1f2db88b971c8da5593b1e94a0ee79d8c892a0f6d4f607d2b77fa23\n","[WARNING|modeling_utils.py:1196] 2021-04-19 21:17:56,134 >> Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.1 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:1207] 2021-04-19 21:17:56,134 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","100% 4/4 [00:00<00:00,  7.47ba/s]\n","100% 1/1 [00:00<00:00, 15.61ba/s]\n","100% 2/2 [00:00<00:00,  7.89ba/s]\n","04/19/2021 21:17:57 - INFO - __main__ -   Sample 2619 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 2916, 'input_ids': [101, 1103, 10830, 1127, 1678, 1146, 1114, 24987, 1149, 13260, 1147, 1692, 1222, 1821, 2180, 5303, 117, 3455, 3081, 5097, 1104, 4961, 1149, 13260, 9966, 1222, 1140, 119, 102, 10830, 1127, 1678, 1146, 1114, 24987, 1149, 13260, 1147, 1692, 1222, 1821, 2180, 5303, 117, 3455, 170, 3081, 118, 3674, 21100, 2998, 1106, 1103, 2175, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1, 'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n","04/19/2021 21:17:57 - INFO - __main__ -   Sample 456 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 509, 'input_ids': [101, 22572, 11252, 1424, 3878, 1684, 1111, 1103, 182, 2155, 2528, 2246, 118, 5534, 1433, 1132, 170, 6539, 4010, 1111, 9283, 1105, 6646, 1110, 1919, 1344, 3075, 1104, 1397, 3336, 6194, 112, 188, 5200, 1728, 1107, 1594, 118, 7820, 22572, 11252, 15449, 119, 102, 3878, 1107, 22572, 11252, 15449, 112, 188, 182, 2155, 2528, 2246, 118, 5534, 1433, 1132, 170, 6539, 4010, 1111, 9283, 117, 1105, 6646, 1110, 1919, 1344, 3075, 1104, 3336, 6194, 112, 188, 5200, 1728, 1107, 1103, 1594, 118, 187, 15677, 3660, 1805, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1, 'sentence1': \"Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .\", 'sentence2': \"Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .\", 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n","04/19/2021 21:17:57 - INFO - __main__ -   Sample 102 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 116, 'input_ids': [101, 2530, 111, 2869, 112, 188, 2260, 4482, 7448, 2174, 1116, 5799, 125, 119, 1969, 1827, 1106, 5103, 1495, 119, 1851, 117, 1229, 9468, 1116, 1810, 4426, 2174, 1116, 2204, 127, 119, 126, 1827, 1106, 122, 117, 20278, 119, 1851, 119, 102, 1103, 2530, 111, 2869, 112, 188, 2260, 7448, 1108, 1146, 122, 119, 3453, 1827, 117, 1137, 121, 119, 1407, 3029, 117, 1106, 5311, 1559, 119, 5599, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 0, 'sentence1': \"Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .\", 'sentence2': \"The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .\", 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n","Downloading: 5.75kB [00:00, 6.88MB/s]       \n","[INFO|trainer.py:497] 2021-04-19 21:18:08,308 >> The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1.\n","[INFO|trainer.py:497] 2021-04-19 21:18:08,308 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1.\n","[INFO|trainer.py:1088] 2021-04-19 21:18:08,497 >> ***** Running training *****\n","[INFO|trainer.py:1089] 2021-04-19 21:18:08,497 >>   Num examples = 3668\n","[INFO|trainer.py:1090] 2021-04-19 21:18:08,498 >>   Num Epochs = 3\n","[INFO|trainer.py:1091] 2021-04-19 21:18:08,498 >>   Instantaneous batch size per device = 32\n","[INFO|trainer.py:1092] 2021-04-19 21:18:08,498 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n","[INFO|trainer.py:1093] 2021-04-19 21:18:08,498 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1094] 2021-04-19 21:18:08,498 >>   Total optimization steps = 345\n","100% 345/345 [02:41<00:00,  2.31it/s][INFO|trainer.py:1273] 2021-04-19 21:20:49,587 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 161.0888, 'train_samples_per_second': 2.142, 'epoch': 3.0}\n","100% 345/345 [02:41<00:00,  2.14it/s]\n","[INFO|trainer.py:1742] 2021-04-19 21:20:49,820 >> Saving model checkpoint to /tmp/mrpc/\n","[INFO|configuration_utils.py:329] 2021-04-19 21:20:49,821 >> Configuration saved in /tmp/mrpc/config.json\n","[INFO|modeling_utils.py:848] 2021-04-19 21:20:50,802 >> Model weights saved in /tmp/mrpc/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1918] 2021-04-19 21:20:50,802 >> tokenizer config file saved in /tmp/mrpc/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1924] 2021-04-19 21:20:50,802 >> Special tokens file saved in /tmp/mrpc/special_tokens_map.json\n","[INFO|trainer_pt_utils.py:898] 2021-04-19 21:20:50,845 >> ***** train metrics *****\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:20:50,845 >>   epoch                      =        3.0\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:20:50,846 >>   init_mem_cpu_alloc_delta   =     1612MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:20:50,846 >>   init_mem_cpu_peaked_delta  =        0MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:20:50,846 >>   init_mem_gpu_alloc_delta   =      413MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:20:50,846 >>   init_mem_gpu_peaked_delta  =        0MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:20:50,846 >>   train_mem_cpu_alloc_delta  =       18MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:20:50,846 >>   train_mem_cpu_peaked_delta =        0MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:20:50,846 >>   train_mem_gpu_alloc_delta  =     1299MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:20:50,846 >>   train_mem_gpu_peaked_delta =     3394MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:20:50,846 >>   train_runtime              = 0:02:41.08\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:20:50,846 >>   train_samples              =       3668\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:20:50,846 >>   train_samples_per_second   =      2.142\n","04/19/2021 21:20:50 - INFO - __main__ -   *** Evaluate ***\n","[INFO|trainer.py:497] 2021-04-19 21:20:50,956 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1.\n","[INFO|trainer.py:1962] 2021-04-19 21:20:50,957 >> ***** Running Evaluation *****\n","[INFO|trainer.py:1964] 2021-04-19 21:20:50,957 >>   Num examples = 408\n","[INFO|trainer.py:1967] 2021-04-19 21:20:50,957 >>   Batch size = 8\n","100% 51/51 [00:01<00:00, 26.00it/s]\n","[INFO|trainer_pt_utils.py:898] 2021-04-19 21:20:53,134 >> ***** eval metrics *****\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:20:53,134 >>   epoch                     =        3.0\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:20:53,134 >>   eval_accuracy             =     0.8676\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:20:53,134 >>   eval_combined_score       =     0.8881\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:20:53,134 >>   eval_f1                   =     0.9085\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:20:53,134 >>   eval_loss                 =     0.3599\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:20:53,134 >>   eval_mem_cpu_alloc_delta  =        1MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:20:53,134 >>   eval_mem_cpu_peaked_delta =        0MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:20:53,134 >>   eval_mem_gpu_alloc_delta  =        0MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:20:53,134 >>   eval_mem_gpu_peaked_delta =       33MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:20:53,134 >>   eval_runtime              = 0:00:02.07\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:20:53,134 >>   eval_samples              =        408\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:20:53,134 >>   eval_samples_per_second   =    196.845\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LtlclLA7Avle","outputId":"71fd4783-6a92-425f-f991-b93fc8697fb5"},"source":["!python run_glue.py \\\n","  --model_name_or_path dmis-lab/biobert-base-cased-v1.1 \\\n","  --task_name cola \\\n","  --do_train \\\n","  --do_eval \\\n","  --max_seq_length 128 \\\n","  --per_device_train_batch_size 32 \\\n","  --learning_rate 2e-5 \\\n","  --num_train_epochs 3 \\\n","  --output_dir /tmp/cola/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-04-19 21:22:40.502333: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n","04/19/2021 21:22:42 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","04/19/2021 21:22:42 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/tmp/cola/, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Apr19_21-22-42_3483064768e1, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/tmp/cola/, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, _n_gpu=1, mp_parameters=)\n","Downloading and preparing dataset glue/cola (download: 368.14 KiB, generated: 596.73 KiB, post-processed: Unknown size, total: 964.86 KiB) to /root/.cache/huggingface/datasets/glue/cola/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4...\n","Downloading: 100% 377k/377k [00:00<00:00, 3.20MB/s]\n","Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/cola/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4. Subsequent calls will reuse this data.\n","[INFO|configuration_utils.py:491] 2021-04-19 21:22:44,126 >> loading configuration file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/78e6e8ece5b58501028ce314273009ad7707ef4c5ba44251914fd6bca8a05eff.e4a2e693122d98b8b56b7dc1f0d89b644226aacef228afb5030ee3621b2829d3\n","[INFO|configuration_utils.py:527] 2021-04-19 21:22:44,127 >> Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": \"cola\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.6.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|configuration_utils.py:491] 2021-04-19 21:22:44,340 >> loading configuration file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/78e6e8ece5b58501028ce314273009ad7707ef4c5ba44251914fd6bca8a05eff.e4a2e693122d98b8b56b7dc1f0d89b644226aacef228afb5030ee3621b2829d3\n","[INFO|configuration_utils.py:527] 2021-04-19 21:22:44,340 >> Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.6.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|tokenization_utils_base.py:1713] 2021-04-19 21:22:45,367 >> loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/cda9c238619f77c4dbf65f23ab841b5239f0289800d673957c843b9dda52ad44.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|tokenization_utils_base.py:1713] 2021-04-19 21:22:45,368 >> loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:1713] 2021-04-19 21:22:45,368 >> loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1713] 2021-04-19 21:22:45,368 >> loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1713] 2021-04-19 21:22:45,368 >> loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/tokenizer_config.json from cache at None\n","[INFO|modeling_utils.py:1075] 2021-04-19 21:22:45,611 >> loading weights file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/f22d3e86464eb4aa2e5f76e0560e9d595617f55e014c78f176b834f9245a62f0.2cf22e1fb1f2db88b971c8da5593b1e94a0ee79d8c892a0f6d4f607d2b77fa23\n","[WARNING|modeling_utils.py:1196] 2021-04-19 21:22:48,891 >> Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.1 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:1207] 2021-04-19 21:22:48,891 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","100% 9/9 [00:00<00:00, 17.20ba/s]\n","100% 2/2 [00:00<00:00, 29.42ba/s]\n","100% 2/2 [00:00<00:00, 11.28ba/s]\n","04/19/2021 21:22:49 - INFO - __main__ -   Sample 1824 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 1824, 'input_ids': [101, 178, 8646, 1115, 1139, 1401, 117, 1119, 1108, 3600, 1112, 1126, 19976, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 0, 'sentence': 'I acknowledged that my father, he was tight as an owl.', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n","04/19/2021 21:22:49 - INFO - __main__ -   Sample 409 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 409, 'input_ids': [101, 1111, 1140, 1106, 1202, 1115, 1156, 1129, 170, 6223, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1, 'sentence': 'For him to do that would be a mistake.', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n","04/19/2021 21:22:49 - INFO - __main__ -   Sample 4506 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 4506, 'input_ids': [101, 12477, 1616, 6407, 170, 1461, 117, 1133, 5837, 1162, 1309, 1225, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1, 'sentence': 'Mary sang a song, but Lee never did.', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n","[INFO|trainer.py:497] 2021-04-19 21:22:53,625 >> The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence.\n","[INFO|trainer.py:497] 2021-04-19 21:22:53,625 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence.\n","[INFO|trainer.py:1088] 2021-04-19 21:22:53,811 >> ***** Running training *****\n","[INFO|trainer.py:1089] 2021-04-19 21:22:53,811 >>   Num examples = 8551\n","[INFO|trainer.py:1090] 2021-04-19 21:22:53,811 >>   Num Epochs = 3\n","[INFO|trainer.py:1091] 2021-04-19 21:22:53,811 >>   Instantaneous batch size per device = 32\n","[INFO|trainer.py:1092] 2021-04-19 21:22:53,811 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n","[INFO|trainer.py:1093] 2021-04-19 21:22:53,812 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1094] 2021-04-19 21:22:53,812 >>   Total optimization steps = 804\n","{'loss': 0.5108, 'learning_rate': 7.5621890547263685e-06, 'epoch': 1.87}\n"," 62% 500/804 [03:51<02:21,  2.14it/s][INFO|trainer.py:1742] 2021-04-19 21:26:45,363 >> Saving model checkpoint to /tmp/cola/checkpoint-500\n","[INFO|configuration_utils.py:329] 2021-04-19 21:26:45,364 >> Configuration saved in /tmp/cola/checkpoint-500/config.json\n","[INFO|modeling_utils.py:848] 2021-04-19 21:26:46,553 >> Model weights saved in /tmp/cola/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1918] 2021-04-19 21:26:46,553 >> tokenizer config file saved in /tmp/cola/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1924] 2021-04-19 21:26:46,554 >> Special tokens file saved in /tmp/cola/checkpoint-500/special_tokens_map.json\n","100% 804/804 [06:17<00:00,  2.60it/s][INFO|trainer.py:1273] 2021-04-19 21:29:11,037 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 377.2257, 'train_samples_per_second': 2.131, 'epoch': 3.0}\n","100% 804/804 [06:17<00:00,  2.13it/s]\n","[INFO|trainer.py:1742] 2021-04-19 21:29:11,274 >> Saving model checkpoint to /tmp/cola/\n","[INFO|configuration_utils.py:329] 2021-04-19 21:29:11,275 >> Configuration saved in /tmp/cola/config.json\n","[INFO|modeling_utils.py:848] 2021-04-19 21:29:12,347 >> Model weights saved in /tmp/cola/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1918] 2021-04-19 21:29:12,347 >> tokenizer config file saved in /tmp/cola/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1924] 2021-04-19 21:29:12,348 >> Special tokens file saved in /tmp/cola/special_tokens_map.json\n","[INFO|trainer_pt_utils.py:898] 2021-04-19 21:29:12,385 >> ***** train metrics *****\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:29:12,385 >>   epoch                      =        3.0\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:29:12,385 >>   init_mem_cpu_alloc_delta   =     1630MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:29:12,385 >>   init_mem_cpu_peaked_delta  =        0MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:29:12,385 >>   init_mem_gpu_alloc_delta   =      413MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:29:12,385 >>   init_mem_gpu_peaked_delta  =        0MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:29:12,385 >>   train_mem_cpu_alloc_delta  =       20MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:29:12,385 >>   train_mem_cpu_peaked_delta =        1MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:29:12,385 >>   train_mem_gpu_alloc_delta  =     1299MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:29:12,385 >>   train_mem_gpu_peaked_delta =     3394MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:29:12,385 >>   train_runtime              = 0:06:17.22\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:29:12,385 >>   train_samples              =       8551\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:29:12,385 >>   train_samples_per_second   =      2.131\n","04/19/2021 21:29:12 - INFO - __main__ -   *** Evaluate ***\n","[INFO|trainer.py:497] 2021-04-19 21:29:12,498 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence.\n","[INFO|trainer.py:1962] 2021-04-19 21:29:12,499 >> ***** Running Evaluation *****\n","[INFO|trainer.py:1964] 2021-04-19 21:29:12,499 >>   Num examples = 1043\n","[INFO|trainer.py:1967] 2021-04-19 21:29:12,499 >>   Batch size = 8\n","100% 131/131 [00:05<00:00, 25.64it/s]\n","[INFO|trainer_pt_utils.py:898] 2021-04-19 21:29:17,754 >> ***** eval metrics *****\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:29:17,754 >>   epoch                     =        3.0\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:29:17,754 >>   eval_loss                 =     0.6011\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:29:17,754 >>   eval_matthews_correlation =     0.3812\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:29:17,755 >>   eval_mem_cpu_alloc_delta  =        2MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:29:17,755 >>   eval_mem_cpu_peaked_delta =        0MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:29:17,755 >>   eval_mem_gpu_alloc_delta  =        0MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:29:17,755 >>   eval_mem_gpu_peaked_delta =       33MB\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:29:17,755 >>   eval_runtime              = 0:00:05.14\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:29:17,755 >>   eval_samples              =       1043\n","[INFO|trainer_pt_utils.py:903] 2021-04-19 21:29:17,755 >>   eval_samples_per_second   =    202.655\n"],"name":"stdout"}]}]}